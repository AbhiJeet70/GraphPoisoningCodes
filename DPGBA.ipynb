{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [],
      "dockerImageVersionId": 30786,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AbhiJeet70/GraphPoisoningCodes/blob/main/DPGBA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install torch-geometric\n",
        "!pip install ogb\n",
        "!pip install matplotlib\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch_geometric.nn import GCNConv, SAGEConv, GATConv\n",
        "from torch_geometric.datasets import Planetoid, Flickr\n",
        "from ogb.nodeproppred import PygNodePropPredDataset\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "import random\n",
        "from torch_geometric.utils import subgraph\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "def set_seed(seed=42):\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "\n",
        "set_seed()\n",
        "\n",
        "# Load datasets\n",
        "def load_dataset(name):\n",
        "    if name in ['Cora', 'PubMed', 'CiteSeer']:\n",
        "        return Planetoid(root=f'/tmp/{name}', name=name)\n",
        "    elif name == 'ogbn-arxiv':\n",
        "        return PygNodePropPredDataset(name='ogbn-arxiv')\n",
        "    elif name == 'Flickr':\n",
        "        return Flickr(root='/tmp/Flickr')\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown dataset: {name}\")\n",
        "\n",
        "# Define GCN Model\n",
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(GCN, self).__init__()\n",
        "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
        "        self.conv2 = GCNConv(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        x = F.relu(self.conv1(x, edge_index))\n",
        "        x = F.dropout(x, training=self.training)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "# Define GraphSAGE Model\n",
        "class GraphSAGE(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(GraphSAGE, self).__init__()\n",
        "        self.conv1 = SAGEConv(input_dim, hidden_dim)\n",
        "        self.conv2 = SAGEConv(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        x = F.relu(self.conv1(x, edge_index))\n",
        "        x = F.dropout(x, training=self.training)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "# Define GAT Model\n",
        "class GAT(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(GAT, self).__init__()\n",
        "        self.conv1 = GATConv(input_dim, hidden_dim, heads=8, dropout=0.6)\n",
        "        self.conv2 = GATConv(hidden_dim * 8, output_dim, heads=1, concat=False, dropout=0.6)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        x = F.relu(self.conv1(x, edge_index))\n",
        "        x = F.dropout(x, training=self.training)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "# Define Trigger Generator (2-layer MLP)\n",
        "class TriggerGenerator(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim):\n",
        "        super(TriggerGenerator, self).__init__()\n",
        "        self.fc1 = torch.nn.Linear(input_dim, hidden_dim)\n",
        "        self.fc2 = torch.nn.Linear(hidden_dim, input_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        return self.fc2(x)\n",
        "\n",
        "# Define OOD Detector (Discriminator)\n",
        "class OODDetector(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim):\n",
        "        super(OODDetector, self).__init__()\n",
        "        self.fc1 = torch.nn.Linear(input_dim, hidden_dim)\n",
        "        self.fc2 = torch.nn.Linear(hidden_dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        return torch.sigmoid(self.fc2(x))\n",
        "\n",
        "# Training function\n",
        "def train(model, data, optimizer):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    out = model(data)\n",
        "    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
        "    loss.backward(retain_graph=True)\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "# Test function\n",
        "def test(model, data):\n",
        "    model.eval()\n",
        "    logits = model(data)\n",
        "    preds = logits.argmax(dim=1)\n",
        "    accs = []\n",
        "    for mask in [data.train_mask, data.val_mask, data.test_mask]:\n",
        "        acc = accuracy_score(data.y[mask].cpu(), preds[mask].cpu())\n",
        "        accs.append(acc)\n",
        "    return accs\n",
        "\n",
        "# Backdoor Attack with Trigger Generator (Adversarial Training with OOD Detector)\n",
        "def add_backdoor(data, trigger_generator, ood_detector, target_class, num_poisoned, adv_epochs=50):\n",
        "    poisoned_indices = random.sample(range(data.num_nodes), num_poisoned)\n",
        "    trigger_optimizer = optim.Adam(trigger_generator.parameters(), lr=0.01)\n",
        "    ood_optimizer = optim.Adam(ood_detector.parameters(), lr=0.01)\n",
        "\n",
        "    # Adversarial training loop\n",
        "    for epoch in range(adv_epochs):\n",
        "        # Train OOD detector to distinguish between real and generated triggers\n",
        "        ood_optimizer.zero_grad()\n",
        "        real_samples = data.x[poisoned_indices]\n",
        "        fake_samples = trigger_generator(real_samples)\n",
        "        real_labels = torch.ones(real_samples.size(0), 1)\n",
        "        fake_labels = torch.zeros(fake_samples.size(0), 1)\n",
        "\n",
        "        ood_real_loss = F.binary_cross_entropy(ood_detector(real_samples), real_labels)\n",
        "        ood_fake_loss = F.binary_cross_entropy(ood_detector(fake_samples), fake_labels)\n",
        "        ood_loss = ood_real_loss + ood_fake_loss\n",
        "        ood_loss.backward()\n",
        "        ood_optimizer.step()\n",
        "\n",
        "        # Train trigger generator to fool the OOD detector\n",
        "        trigger_optimizer.zero_grad()\n",
        "        fake_samples = trigger_generator(real_samples)\n",
        "        fool_labels = torch.ones(fake_samples.size(0), 1)\n",
        "        generator_loss = F.binary_cross_entropy(ood_detector(fake_samples), fool_labels)\n",
        "        generator_loss.backward()\n",
        "        trigger_optimizer.step()\n",
        "\n",
        "        if epoch % 10 == 0:\n",
        "            print(f\"Adversarial Epoch {epoch}, OOD Loss: {ood_loss.item():.4f}, Generator Loss: {generator_loss.item():.4f}\")\n",
        "\n",
        "    # Apply the generated trigger to the poisoned nodes\n",
        "    trigger_node_features = trigger_generator(data.x[poisoned_indices])\n",
        "    data.x[poisoned_indices] = trigger_node_features\n",
        "    data.y[poisoned_indices] = target_class\n",
        "    return data, poisoned_indices\n",
        "\n",
        "# Outlier Detection (using the trained OOD detector)\n",
        "def apply_outlier_detection(data, ood_detector, threshold=0.5):\n",
        "    # Use the OOD detector to identify outliers\n",
        "    with torch.no_grad():\n",
        "        ood_scores = ood_detector(data.x)\n",
        "    mask = ood_scores <= threshold\n",
        "\n",
        "    # Apply mask to nodes and adjust edges accordingly\n",
        "    data.x = data.x[mask.squeeze()]\n",
        "    data.y = data.y[mask.squeeze()]\n",
        "    data.edge_index, _ = subgraph(mask.squeeze(), data.edge_index, relabel_nodes=True, num_nodes=data.num_nodes)\n",
        "\n",
        "    # Update the train, val, and test masks accordingly\n",
        "    if hasattr(data, 'train_mask'):\n",
        "        data.train_mask = data.train_mask[mask.squeeze()]\n",
        "    if hasattr(data, 'val_mask'):\n",
        "        data.val_mask = data.val_mask[mask.squeeze()]\n",
        "    if hasattr(data, 'test_mask'):\n",
        "        data.test_mask = data.test_mask[mask.squeeze()]\n",
        "\n",
        "    # Update poisoned indices to reflect only nodes that are still in the dataset\n",
        "    mask_indices = torch.nonzero(mask.squeeze(), as_tuple=True)[0]\n",
        "    return data, mask_indices\n",
        "\n",
        "# Main experiment loop\n",
        "def run_experiment():\n",
        "    # Datasets to evaluate\n",
        "    datasets = ['Cora', 'PubMed', 'Flickr', 'ogbn-arxiv']\n",
        "    attack_budgets = {'Cora': 10, 'PubMed': 40, 'Flickr': 160, 'ogbn-arxiv': 565}\n",
        "    trigger_size = 3  # Trigger size limit\n",
        "\n",
        "    # Hyperparameters\n",
        "    hidden_dim = 16\n",
        "    learning_rate = 0.01\n",
        "    epochs = 200\n",
        "    target_class = 1  # The class label to target in the backdoor attack\n",
        "\n",
        "    # Define models to evaluate\n",
        "    models = {'GCN': GCN, 'GraphSAGE': GraphSAGE, 'GAT': GAT}\n",
        "\n",
        "    # Store all results for final comparison table\n",
        "    all_results = []\n",
        "\n",
        "    # Run experiments for each dataset\n",
        "    for dataset_name in datasets:\n",
        "        print(f\"Running experiment on dataset: {dataset_name}\")\n",
        "        dataset = load_dataset(dataset_name)\n",
        "        data = dataset[0]\n",
        "\n",
        "        # Create train, validation, and test masks if they do not exist\n",
        "        if not hasattr(data, 'train_mask'):\n",
        "            num_nodes = data.num_nodes\n",
        "            train_ratio, val_ratio = 0.1, 0.1\n",
        "            train_size = int(train_ratio * num_nodes)\n",
        "            val_size = int(val_ratio * num_nodes)\n",
        "            test_size = num_nodes - train_size - val_size\n",
        "\n",
        "            perm = torch.randperm(num_nodes)\n",
        "            data.train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
        "            data.val_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
        "            data.test_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
        "\n",
        "            data.train_mask[perm[:train_size]] = True\n",
        "            data.val_mask[perm[train_size:train_size + val_size]] = True\n",
        "            data.test_mask[perm[train_size + val_size:]] = True\n",
        "        input_dim = dataset.num_node_features\n",
        "        output_dim = dataset.num_classes\n",
        "        num_poisoned = attack_budgets[dataset_name]\n",
        "\n",
        "        # Initialize trigger generator and OOD detector\n",
        "        trigger_generator = TriggerGenerator(input_dim, hidden_dim)\n",
        "        ood_detector = OODDetector(input_dim, hidden_dim)\n",
        "\n",
        "        # Run experiments for each model\n",
        "        for model_name, ModelClass in models.items():\n",
        "            print(f\"Running experiment for {model_name}\")\n",
        "            model = ModelClass(input_dim, hidden_dim, output_dim)\n",
        "            optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "            # Train on clean data\n",
        "            for epoch in range(epochs):\n",
        "                loss = train(model, data, optimizer)\n",
        "                if epoch % 10 == 0:\n",
        "                    print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
        "\n",
        "            # Test on clean data\n",
        "            clean_accs = test(model, data)\n",
        "            print(f\"{model_name} Clean Data Accuracy: Train: {clean_accs[0]:.4f}, Val: {clean_accs[1]:.4f}, Test: {clean_accs[2]:.4f}\")\n",
        "\n",
        "            # Add backdoor trigger with adversarial training\n",
        "            poisoned_data, poisoned_indices = add_backdoor(data.clone(), trigger_generator, ood_detector, target_class, num_poisoned)\n",
        "\n",
        "            # Retrain on poisoned data (No Defense)\n",
        "            for epoch in range(epochs):\n",
        "                loss = train(model, poisoned_data, optimizer)\n",
        "                if epoch % 10 == 0:\n",
        "                    print(f\"Epoch {epoch} (Poisoned), Loss: {loss:.4f}\")\n",
        "\n",
        "            # Test on poisoned data (No Defense)\n",
        "            poisoned_accs = test(model, poisoned_data)\n",
        "            poisoned_preds = model(poisoned_data).argmax(dim=1)\n",
        "            asr = (poisoned_preds[poisoned_indices] == target_class).float().mean().item()\n",
        "            print(f\"{model_name} No Defense - Poisoned Data Accuracy: Train: {poisoned_accs[0]:.4f}, Val: {poisoned_accs[1]:.4f}, Test: {poisoned_accs[2]:.4f}, ASR: {asr:.4f}\")\n",
        "\n",
        "            # Apply outlier detection defense\n",
        "            defended_data, mask_indices = apply_outlier_detection(poisoned_data.clone(), ood_detector)\n",
        "            valid_poisoned_indices = [mask_indices.tolist().index(i) for i in poisoned_indices if i in mask_indices]\n",
        "\n",
        "            # Retrain on defended data\n",
        "            for epoch in range(epochs):\n",
        "                loss = train(model, defended_data, optimizer)\n",
        "                if epoch % 10 == 0:\n",
        "                    print(f\"Epoch {epoch} (Defended), Loss: {loss:.4f}\")\n",
        "\n",
        "            # Test on defended data\n",
        "            defended_accs = test(model, defended_data)\n",
        "            defended_preds = model(defended_data).argmax(dim=1)\n",
        "            defended_asr = (defended_preds[valid_poisoned_indices] == target_class).float().mean().item() if len(valid_poisoned_indices) > 0 else 0.0\n",
        "            print(f\"{model_name} Outlier Detection - Defended Data Accuracy: Train: {defended_accs[0]:.4f}, Val: {defended_accs[1]:.4f}, Test: {defended_accs[2]:.4f}, ASR: {defended_asr:.4f}\")\n",
        "\n",
        "            # Store results for comparison\n",
        "            all_results.append({\n",
        "                'Dataset': dataset_name,\n",
        "                'Model': model_name,\n",
        "                'Clean Test Accuracy': clean_accs[2],\n",
        "                'Poisoned Test Accuracy (No Defense)': poisoned_accs[2],\n",
        "                'Defended Test Accuracy (Outlier Detection)': defended_accs[2],\n",
        "                'Attack Success Rate (No Defense)': asr,\n",
        "                'Attack Success Rate (Outlier Detection)': defended_asr\n",
        "            })\n",
        "\n",
        "    # Create a DataFrame for the results\n",
        "    results_df = pd.DataFrame(all_results)\n",
        "    print(\"Final Comparison Table:\")\n",
        "    print(results_df)\n",
        "\n",
        "    # Save the results to an Excel file\n",
        "    results_df.to_excel('experiment_results.xlsx', index=False)\n",
        "\n",
        "    # Plot the results\n",
        "    fig, axes = plt.subplots(2, 1, figsize=(12, 12))\n",
        "\n",
        "    # Plot Clean, Poisoned, and Defended Accuracies\n",
        "    for dataset_name in datasets:\n",
        "        subset = results_df[results_df['Dataset'] == dataset_name]\n",
        "        axes[0].plot(subset['Model'], subset['Clean Test Accuracy'], marker='o', label=f'{dataset_name} - Clean')\n",
        "        axes[0].plot(subset['Model'], subset['Poisoned Test Accuracy (No Defense)'], marker='x', linestyle='--', label=f'{dataset_name} - Poisoned (No Defense)')\n",
        "        axes[0].plot(subset['Model'], subset['Defended Test Accuracy (Outlier Detection)'], marker='^', linestyle='-.', label=f'{dataset_name} - Defended')\n",
        "    axes[0].set_title('Clean, Poisoned, and Defended Test Accuracies')\n",
        "    axes[0].set_xlabel('Model')\n",
        "    axes[0].set_ylabel('Accuracy')\n",
        "    axes[0].legend()\n",
        "    axes[0].grid(True)\n",
        "\n",
        "    # Plot Attack Success Rates\n",
        "    for dataset_name in datasets:\n",
        "        subset = results_df[results_df['Dataset'] == dataset_name]\n",
        "        axes[1].plot(subset['Model'], subset['Attack Success Rate (No Defense)'], marker='o', linestyle='-', label=f'{dataset_name} - ASR (No Defense)')\n",
        "        axes[1].plot(subset['Model'], subset['Attack Success Rate (Outlier Detection)'], marker='x', linestyle='--', label=f'{dataset_name} - ASR (Outlier Detection)')\n",
        "    axes[1].set_title('Attack Success Rates')\n",
        "    axes[1].set_xlabel('Model')\n",
        "    axes[1].set_ylabel('Attack Success Rate')\n",
        "    axes[1].legend()\n",
        "    axes[1].grid(True)\n",
        "\n",
        "    # Show the plots\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_experiment()"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2024-10-10T08:48:28.893171Z",
          "iopub.execute_input": "2024-10-10T08:48:28.893732Z",
          "iopub.status.idle": "2024-10-10T09:33:25.802651Z",
          "shell.execute_reply.started": "2024-10-10T08:48:28.893687Z",
          "shell.execute_reply": "2024-10-10T09:33:25.800429Z"
        },
        "trusted": true,
        "id": "RVaRVIdqem5Q",
        "outputId": "d3a383f8-b5f8-4598-ffd6-29495d9feaaa"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Requirement already satisfied: torch-geometric in /opt/conda/lib/python3.10/site-packages (2.6.1)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from torch-geometric) (3.9.5)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch-geometric) (2024.6.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch-geometric) (3.1.4)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torch-geometric) (1.26.4)\nRequirement already satisfied: psutil>=5.8.0 in /opt/conda/lib/python3.10/site-packages (from torch-geometric) (5.9.3)\nRequirement already satisfied: pyparsing in /opt/conda/lib/python3.10/site-packages (from torch-geometric) (3.1.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torch-geometric) (2.32.3)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from torch-geometric) (4.66.4)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch-geometric) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch-geometric) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch-geometric) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch-geometric) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch-geometric) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch-geometric) (4.0.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch-geometric) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torch-geometric) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torch-geometric) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torch-geometric) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torch-geometric) (2024.8.30)\nRequirement already satisfied: ogb in /opt/conda/lib/python3.10/site-packages (1.3.6)\nRequirement already satisfied: torch>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from ogb) (2.4.0+cpu)\nRequirement already satisfied: numpy>=1.16.0 in /opt/conda/lib/python3.10/site-packages (from ogb) (1.26.4)\nRequirement already satisfied: tqdm>=4.29.0 in /opt/conda/lib/python3.10/site-packages (from ogb) (4.66.4)\nRequirement already satisfied: scikit-learn>=0.20.0 in /opt/conda/lib/python3.10/site-packages (from ogb) (1.2.2)\nRequirement already satisfied: pandas>=0.24.0 in /opt/conda/lib/python3.10/site-packages (from ogb) (2.2.3)\nRequirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from ogb) (1.16.0)\nRequirement already satisfied: urllib3>=1.24.0 in /opt/conda/lib/python3.10/site-packages (from ogb) (1.26.18)\nRequirement already satisfied: outdated>=0.2.0 in /opt/conda/lib/python3.10/site-packages (from ogb) (0.2.2)\nRequirement already satisfied: setuptools>=44 in /opt/conda/lib/python3.10/site-packages (from outdated>=0.2.0->ogb) (70.0.0)\nRequirement already satisfied: littleutils in /opt/conda/lib/python3.10/site-packages (from outdated>=0.2.0->ogb) (0.2.4)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from outdated>=0.2.0->ogb) (2.32.3)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas>=0.24.0->ogb) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=0.24.0->ogb) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas>=0.24.0->ogb) (2024.1)\nRequirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.20.0->ogb) (1.14.1)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.20.0->ogb) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.20.0->ogb) (3.5.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->ogb) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->ogb) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->ogb) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->ogb) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->ogb) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->ogb) (2024.6.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.6.0->ogb) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->outdated>=0.2.0->ogb) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->outdated>=0.2.0->ogb) (3.7)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->outdated>=0.2.0->ogb) (2024.8.30)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.6.0->ogb) (1.3.0)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (3.7.5)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (1.2.1)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (4.53.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (1.4.5)\nRequirement already satisfied: numpy<2,>=1.20 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (21.3)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (10.3.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (3.1.2)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (2.9.0.post0)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\nRunning experiment on dataset: Cora\nRunning experiment for GCN\nEpoch 0, Loss: 1.9414\nEpoch 10, Loss: 0.6484\nEpoch 20, Loss: 0.2111\nEpoch 30, Loss: 0.0893\nEpoch 40, Loss: 0.0685\nEpoch 50, Loss: 0.0302\nEpoch 60, Loss: 0.0195\nEpoch 70, Loss: 0.0253\nEpoch 80, Loss: 0.0153\nEpoch 90, Loss: 0.0140\nEpoch 100, Loss: 0.0105\nEpoch 110, Loss: 0.0111\nEpoch 120, Loss: 0.0050\nEpoch 130, Loss: 0.0091\nEpoch 140, Loss: 0.0085\nEpoch 150, Loss: 0.0110\nEpoch 160, Loss: 0.0158\nEpoch 170, Loss: 0.0207\nEpoch 180, Loss: 0.0102\nEpoch 190, Loss: 0.0078\nGCN Clean Data Accuracy: Train: 1.0000, Val: 0.7440, Test: 0.7750\nAdversarial Epoch 0, OOD Loss: 1.3888, Generator Loss: 1.7817\nAdversarial Epoch 10, OOD Loss: 1.3275, Generator Loss: 0.9446\nAdversarial Epoch 20, OOD Loss: 0.4477, Generator Loss: 9.6050\nAdversarial Epoch 30, OOD Loss: 0.1584, Generator Loss: 9.3286\nAdversarial Epoch 40, OOD Loss: 0.1510, Generator Loss: 13.3119\nEpoch 0 (Poisoned), Loss: 0.0106\nEpoch 10 (Poisoned), Loss: 0.0036\nEpoch 20 (Poisoned), Loss: 0.0078\nEpoch 30 (Poisoned), Loss: 0.0064\nEpoch 40 (Poisoned), Loss: 0.0088\nEpoch 50 (Poisoned), Loss: 0.0067\nEpoch 60 (Poisoned), Loss: 0.0026\nEpoch 70 (Poisoned), Loss: 0.0027\nEpoch 80 (Poisoned), Loss: 0.0178\nEpoch 90 (Poisoned), Loss: 0.0077\nEpoch 100 (Poisoned), Loss: 0.0018\nEpoch 110 (Poisoned), Loss: 0.0247\nEpoch 120 (Poisoned), Loss: 0.0010\nEpoch 130 (Poisoned), Loss: 0.0016\nEpoch 140 (Poisoned), Loss: 0.0035\nEpoch 150 (Poisoned), Loss: 0.0034\nEpoch 160 (Poisoned), Loss: 0.0199\nEpoch 170 (Poisoned), Loss: 0.0028\nEpoch 180 (Poisoned), Loss: 0.0041\nEpoch 190 (Poisoned), Loss: 0.0078\nGCN No Defense - Poisoned Data Accuracy: Train: 1.0000, Val: 0.7420, Test: 0.7660, ASR: 0.5000\nEpoch 0 (Defended), Loss: 0.5474\nEpoch 10 (Defended), Loss: 0.2334\nEpoch 20 (Defended), Loss: 0.3868\nEpoch 30 (Defended), Loss: 0.0003\nEpoch 40 (Defended), Loss: 0.1901\nEpoch 50 (Defended), Loss: 2.0719\nEpoch 60 (Defended), Loss: 0.0852\nEpoch 70 (Defended), Loss: 0.0708\nEpoch 80 (Defended), Loss: 0.0621\nEpoch 90 (Defended), Loss: 0.0062\nEpoch 100 (Defended), Loss: 0.0000\nEpoch 110 (Defended), Loss: 0.0999\nEpoch 120 (Defended), Loss: 0.0028\nEpoch 130 (Defended), Loss: 0.2617\nEpoch 140 (Defended), Loss: 0.0144\nEpoch 150 (Defended), Loss: 0.0581\nEpoch 160 (Defended), Loss: 0.1527\nEpoch 170 (Defended), Loss: 0.1710\nEpoch 180 (Defended), Loss: 0.0001\nEpoch 190 (Defended), Loss: 0.1070\nGCN Outlier Detection - Defended Data Accuracy: Train: 1.0000, Val: 0.4675, Test: 0.5528, ASR: 1.0000\nRunning experiment for GraphSAGE\nEpoch 0, Loss: 1.9546\nEpoch 10, Loss: 0.3946\nEpoch 20, Loss: 0.0492\nEpoch 30, Loss: 0.0330\nEpoch 40, Loss: 0.0133\nEpoch 50, Loss: 0.0090\nEpoch 60, Loss: 0.0099\nEpoch 70, Loss: 0.0075\nEpoch 80, Loss: 0.0047\nEpoch 90, Loss: 0.0159\nEpoch 100, Loss: 0.0074\nEpoch 110, Loss: 0.0204\nEpoch 120, Loss: 0.0017\nEpoch 130, Loss: 0.0036\nEpoch 140, Loss: 0.0058\nEpoch 150, Loss: 0.0034\nEpoch 160, Loss: 0.0025\nEpoch 170, Loss: 0.0059\nEpoch 180, Loss: 0.0050\nEpoch 190, Loss: 0.0020\nGraphSAGE Clean Data Accuracy: Train: 1.0000, Val: 0.7360, Test: 0.7610\nAdversarial Epoch 0, OOD Loss: 0.5409, Generator Loss: 14.1195\nAdversarial Epoch 10, OOD Loss: 0.0954, Generator Loss: 6.0427\nAdversarial Epoch 20, OOD Loss: 0.6241, Generator Loss: 3.7560\nAdversarial Epoch 30, OOD Loss: 0.1705, Generator Loss: 8.5396\nAdversarial Epoch 40, OOD Loss: 0.5709, Generator Loss: 2.4357\nEpoch 0 (Poisoned), Loss: 0.4028\nEpoch 10 (Poisoned), Loss: 0.0399\nEpoch 20 (Poisoned), Loss: 0.0056\nEpoch 30 (Poisoned), Loss: 0.0589\nEpoch 40 (Poisoned), Loss: 0.0049\nEpoch 50 (Poisoned), Loss: 0.0030\nEpoch 60 (Poisoned), Loss: 0.0114\nEpoch 70 (Poisoned), Loss: 0.0029\nEpoch 80 (Poisoned), Loss: 0.0020\nEpoch 90 (Poisoned), Loss: 0.0021\nEpoch 100 (Poisoned), Loss: 0.0251\nEpoch 110 (Poisoned), Loss: 0.0014\nEpoch 120 (Poisoned), Loss: 0.0108\nEpoch 130 (Poisoned), Loss: 0.0010\nEpoch 140 (Poisoned), Loss: 0.0278\nEpoch 150 (Poisoned), Loss: 0.0184\nEpoch 160 (Poisoned), Loss: 0.0013\nEpoch 170 (Poisoned), Loss: 0.0113\nEpoch 180 (Poisoned), Loss: 0.0015\nEpoch 190 (Poisoned), Loss: 0.0011\nGraphSAGE No Defense - Poisoned Data Accuracy: Train: 1.0000, Val: 0.7320, Test: 0.7440, ASR: 1.0000\nEpoch 0 (Defended), Loss: 0.0066\nEpoch 10 (Defended), Loss: 0.0003\nEpoch 20 (Defended), Loss: 0.0136\nEpoch 30 (Defended), Loss: 0.0012\nEpoch 40 (Defended), Loss: 0.0031\nEpoch 50 (Defended), Loss: 0.0048\nEpoch 60 (Defended), Loss: 0.0102\nEpoch 70 (Defended), Loss: 0.0015\nEpoch 80 (Defended), Loss: 0.0131\nEpoch 90 (Defended), Loss: 0.0008\nEpoch 100 (Defended), Loss: 0.0003\nEpoch 110 (Defended), Loss: 0.0017\nEpoch 120 (Defended), Loss: 0.0071\nEpoch 130 (Defended), Loss: 0.0044\nEpoch 140 (Defended), Loss: 0.0019\nEpoch 150 (Defended), Loss: 0.0120\nEpoch 160 (Defended), Loss: 0.0011\nEpoch 170 (Defended), Loss: 0.0014\nEpoch 180 (Defended), Loss: 0.0096\nEpoch 190 (Defended), Loss: 0.0011\nGraphSAGE Outlier Detection - Defended Data Accuracy: Train: 1.0000, Val: 0.7121, Test: 0.7484, ASR: 0.9000\nRunning experiment for GAT\nEpoch 0, Loss: 1.9537\nEpoch 10, Loss: 0.6066\nEpoch 20, Loss: 0.3947\nEpoch 30, Loss: 0.3252\nEpoch 40, Loss: 0.3233\nEpoch 50, Loss: 0.3244\nEpoch 60, Loss: 0.3412\nEpoch 70, Loss: 0.2631\nEpoch 80, Loss: 0.2703\nEpoch 90, Loss: 0.3118\nEpoch 100, Loss: 0.3096\nEpoch 110, Loss: 0.3371\nEpoch 120, Loss: 0.2219\nEpoch 130, Loss: 0.3589\nEpoch 140, Loss: 0.2960\nEpoch 150, Loss: 0.3218\nEpoch 160, Loss: 0.2668\nEpoch 170, Loss: 0.2709\nEpoch 180, Loss: 0.3459\nEpoch 190, Loss: 0.2948\nGAT Clean Data Accuracy: Train: 1.0000, Val: 0.7660, Test: 0.7990\nAdversarial Epoch 0, OOD Loss: 1.9726, Generator Loss: 5.1267\nAdversarial Epoch 10, OOD Loss: 0.5089, Generator Loss: 7.9239\nAdversarial Epoch 20, OOD Loss: 0.2754, Generator Loss: 12.7046\nAdversarial Epoch 30, OOD Loss: 0.1983, Generator Loss: 9.9509\nAdversarial Epoch 40, OOD Loss: 0.1789, Generator Loss: 6.2993\nEpoch 0 (Poisoned), Loss: 0.4603\nEpoch 10 (Poisoned), Loss: 0.3136\nEpoch 20 (Poisoned), Loss: 0.2924\nEpoch 30 (Poisoned), Loss: 0.3553\nEpoch 40 (Poisoned), Loss: 0.4384\nEpoch 50 (Poisoned), Loss: 0.2572\nEpoch 60 (Poisoned), Loss: 0.3145\nEpoch 70 (Poisoned), Loss: 0.2276\nEpoch 80 (Poisoned), Loss: 0.3983\nEpoch 90 (Poisoned), Loss: 0.3846\nEpoch 100 (Poisoned), Loss: 0.2752\nEpoch 110 (Poisoned), Loss: 0.3065\nEpoch 120 (Poisoned), Loss: 0.2601\nEpoch 130 (Poisoned), Loss: 0.2833\nEpoch 140 (Poisoned), Loss: 0.3519\nEpoch 150 (Poisoned), Loss: 0.2881\nEpoch 160 (Poisoned), Loss: 0.3013\nEpoch 170 (Poisoned), Loss: 0.2920\nEpoch 180 (Poisoned), Loss: 0.2839\nEpoch 190 (Poisoned), Loss: 0.3473\nGAT No Defense - Poisoned Data Accuracy: Train: 0.9929, Val: 0.7560, Test: 0.7700, ASR: 0.0000\nEpoch 0 (Defended), Loss: 0.3233\nEpoch 10 (Defended), Loss: 0.3711\nEpoch 20 (Defended), Loss: 0.3869\nEpoch 30 (Defended), Loss: 0.4118\nEpoch 40 (Defended), Loss: 0.4548\nEpoch 50 (Defended), Loss: 0.3938\nEpoch 60 (Defended), Loss: 0.3659\nEpoch 70 (Defended), Loss: 0.3842\nEpoch 80 (Defended), Loss: 0.3615\nEpoch 90 (Defended), Loss: 0.3471\nEpoch 100 (Defended), Loss: 0.4119\nEpoch 110 (Defended), Loss: 0.4227\nEpoch 120 (Defended), Loss: 0.2857\nEpoch 130 (Defended), Loss: 0.4774\nEpoch 140 (Defended), Loss: 0.4150\nEpoch 150 (Defended), Loss: 0.4845\nEpoch 160 (Defended), Loss: 0.3898\nEpoch 170 (Defended), Loss: 0.4337\nEpoch 180 (Defended), Loss: 0.4465\nEpoch 190 (Defended), Loss: 0.2191\nGAT Outlier Detection - Defended Data Accuracy: Train: 1.0000, Val: 0.7204, Test: 0.7677, ASR: 0.0000\nRunning experiment on dataset: PubMed\nRunning experiment for GCN\nEpoch 0, Loss: 1.1005\nEpoch 10, Loss: 0.9246\nEpoch 20, Loss: 0.7089\nEpoch 30, Loss: 0.4546\nEpoch 40, Loss: 0.3026\nEpoch 50, Loss: 0.1987\nEpoch 60, Loss: 0.1754\nEpoch 70, Loss: 0.1297\nEpoch 80, Loss: 0.1079\nEpoch 90, Loss: 0.0666\nEpoch 100, Loss: 0.0676\nEpoch 110, Loss: 0.0391\nEpoch 120, Loss: 0.0692\nEpoch 130, Loss: 0.0414\nEpoch 140, Loss: 0.0285\nEpoch 150, Loss: 0.0275\nEpoch 160, Loss: 0.0245\nEpoch 170, Loss: 0.0165\nEpoch 180, Loss: 0.0163\nEpoch 190, Loss: 0.0153\nGCN Clean Data Accuracy: Train: 1.0000, Val: 0.7700, Test: 0.7620\nAdversarial Epoch 0, OOD Loss: 1.3947, Generator Loss: 0.8523\nAdversarial Epoch 10, OOD Loss: 1.5021, Generator Loss: 0.7320\nAdversarial Epoch 20, OOD Loss: 0.6361, Generator Loss: 3.1835\nAdversarial Epoch 30, OOD Loss: 2.6657, Generator Loss: 0.2202\nAdversarial Epoch 40, OOD Loss: 0.7017, Generator Loss: 4.0486\nEpoch 0 (Poisoned), Loss: 0.0103\nEpoch 10 (Poisoned), Loss: 0.0148\nEpoch 20 (Poisoned), Loss: 0.0274\nEpoch 30 (Poisoned), Loss: 0.0139\nEpoch 40 (Poisoned), Loss: 0.0127\nEpoch 50 (Poisoned), Loss: 0.0093\nEpoch 60 (Poisoned), Loss: 0.0099\nEpoch 70 (Poisoned), Loss: 0.0157\nEpoch 80 (Poisoned), Loss: 0.0077\nEpoch 90 (Poisoned), Loss: 0.0145\nEpoch 100 (Poisoned), Loss: 0.0084\nEpoch 110 (Poisoned), Loss: 0.0111\nEpoch 120 (Poisoned), Loss: 0.0099\nEpoch 130 (Poisoned), Loss: 0.0096\nEpoch 140 (Poisoned), Loss: 0.0100\nEpoch 150 (Poisoned), Loss: 0.0086\nEpoch 160 (Poisoned), Loss: 0.0020\nEpoch 170 (Poisoned), Loss: 0.0035\nEpoch 180 (Poisoned), Loss: 0.0104\nEpoch 190 (Poisoned), Loss: 0.0093\nGCN No Defense - Poisoned Data Accuracy: Train: 1.0000, Val: 0.7640, Test: 0.7610, ASR: 0.9750\nEpoch 0 (Defended), Loss: 0.0793\nEpoch 10 (Defended), Loss: 0.0000\nEpoch 20 (Defended), Loss: 0.0439\nEpoch 30 (Defended), Loss: 0.0006\nEpoch 40 (Defended), Loss: 0.0006\nEpoch 50 (Defended), Loss: 0.0002\nEpoch 60 (Defended), Loss: 0.0355\nEpoch 70 (Defended), Loss: 0.0151\nEpoch 80 (Defended), Loss: 0.0001\nEpoch 90 (Defended), Loss: 0.0001\nEpoch 100 (Defended), Loss: 0.0001\nEpoch 110 (Defended), Loss: 0.0025\nEpoch 120 (Defended), Loss: 0.0007\nEpoch 130 (Defended), Loss: 0.0003\nEpoch 140 (Defended), Loss: 0.0001\nEpoch 150 (Defended), Loss: 0.0014\nEpoch 160 (Defended), Loss: 0.0001\nEpoch 170 (Defended), Loss: 0.0171\nEpoch 180 (Defended), Loss: 0.0000\nEpoch 190 (Defended), Loss: 0.0001\nGCN Outlier Detection - Defended Data Accuracy: Train: 1.0000, Val: 0.7458, Test: 0.7264, ASR: 1.0000\nRunning experiment for GraphSAGE\nEpoch 0, Loss: 1.1083\nEpoch 10, Loss: 0.7927\nEpoch 20, Loss: 0.3981\nEpoch 30, Loss: 0.1108\nEpoch 40, Loss: 0.0874\nEpoch 50, Loss: 0.0447\nEpoch 60, Loss: 0.0276\nEpoch 70, Loss: 0.0278\nEpoch 80, Loss: 0.0307\nEpoch 90, Loss: 0.0067\nEpoch 100, Loss: 0.0114\nEpoch 110, Loss: 0.0078\nEpoch 120, Loss: 0.0039\nEpoch 130, Loss: 0.0064\nEpoch 140, Loss: 0.0071\nEpoch 150, Loss: 0.0104\nEpoch 160, Loss: 0.0069\nEpoch 170, Loss: 0.0059\nEpoch 180, Loss: 0.0036\nEpoch 190, Loss: 0.0109\nGraphSAGE Clean Data Accuracy: Train: 1.0000, Val: 0.7820, Test: 0.7490\nAdversarial Epoch 0, OOD Loss: 0.8796, Generator Loss: 2.3745\nAdversarial Epoch 10, OOD Loss: 0.6370, Generator Loss: 3.9216\nAdversarial Epoch 20, OOD Loss: 0.6809, Generator Loss: 2.8378\nAdversarial Epoch 30, OOD Loss: 0.5332, Generator Loss: 5.2064\nAdversarial Epoch 40, OOD Loss: 1.2383, Generator Loss: 0.8460\nEpoch 0 (Poisoned), Loss: 0.0058\nEpoch 10 (Poisoned), Loss: 0.0029\nEpoch 20 (Poisoned), Loss: 0.0039\nEpoch 30 (Poisoned), Loss: 0.0012\nEpoch 40 (Poisoned), Loss: 0.0070\nEpoch 50 (Poisoned), Loss: 0.0018\nEpoch 60 (Poisoned), Loss: 0.0093\nEpoch 70 (Poisoned), Loss: 0.0013\nEpoch 80 (Poisoned), Loss: 0.0030\nEpoch 90 (Poisoned), Loss: 0.0023\nEpoch 100 (Poisoned), Loss: 0.0020\nEpoch 110 (Poisoned), Loss: 0.0016\nEpoch 120 (Poisoned), Loss: 0.0024\nEpoch 130 (Poisoned), Loss: 0.0015\nEpoch 140 (Poisoned), Loss: 0.0030\nEpoch 150 (Poisoned), Loss: 0.0015\nEpoch 160 (Poisoned), Loss: 0.0040\nEpoch 170 (Poisoned), Loss: 0.0045\nEpoch 180 (Poisoned), Loss: 0.0051\nEpoch 190 (Poisoned), Loss: 0.0110\nGraphSAGE No Defense - Poisoned Data Accuracy: Train: 1.0000, Val: 0.7820, Test: 0.7530, ASR: 0.3000\nEpoch 0 (Defended), Loss: nan\nEpoch 10 (Defended), Loss: nan\nEpoch 20 (Defended), Loss: nan\nEpoch 30 (Defended), Loss: nan\nEpoch 40 (Defended), Loss: nan\nEpoch 50 (Defended), Loss: nan\nEpoch 60 (Defended), Loss: nan\nEpoch 70 (Defended), Loss: nan\nEpoch 80 (Defended), Loss: nan\nEpoch 90 (Defended), Loss: nan\nEpoch 100 (Defended), Loss: nan\nEpoch 110 (Defended), Loss: nan\nEpoch 120 (Defended), Loss: nan\nEpoch 130 (Defended), Loss: nan\nEpoch 140 (Defended), Loss: nan\nEpoch 150 (Defended), Loss: nan\nEpoch 160 (Defended), Loss: nan\nEpoch 170 (Defended), Loss: nan\nEpoch 180 (Defended), Loss: nan\nEpoch 190 (Defended), Loss: nan\nGraphSAGE Outlier Detection - Defended Data Accuracy: Train: nan, Val: 0.0000, Test: 0.0000, ASR: 0.0000\nRunning experiment for GAT\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/numpy/lib/function_base.py:520: RuntimeWarning: Mean of empty slice.\n  avg = a.mean(axis, **keepdims_kw)\n/opt/conda/lib/python3.10/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n  ret = ret.dtype.type(ret / rcount)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 0, Loss: 1.0940\nEpoch 10, Loss: 0.6840\nEpoch 20, Loss: 0.5111\nEpoch 30, Loss: 0.3855\nEpoch 40, Loss: 0.4200\nEpoch 50, Loss: 0.2304\nEpoch 60, Loss: 0.2168\nEpoch 70, Loss: 0.3345\nEpoch 80, Loss: 0.1779\nEpoch 90, Loss: 0.2423\nEpoch 100, Loss: 0.2420\nEpoch 110, Loss: 0.3326\nEpoch 120, Loss: 0.2048\nEpoch 130, Loss: 0.1984\nEpoch 140, Loss: 0.2583\nEpoch 150, Loss: 0.2142\nEpoch 160, Loss: 0.2089\nEpoch 170, Loss: 0.2141\nEpoch 180, Loss: 0.2038\nEpoch 190, Loss: 0.2221\nGAT Clean Data Accuracy: Train: 1.0000, Val: 0.7580, Test: 0.7580\nAdversarial Epoch 0, OOD Loss: 0.6100, Generator Loss: 3.6418\nAdversarial Epoch 10, OOD Loss: 0.5126, Generator Loss: 3.2682\nAdversarial Epoch 20, OOD Loss: 0.6106, Generator Loss: 2.3010\nAdversarial Epoch 30, OOD Loss: 0.4740, Generator Loss: 3.4712\nAdversarial Epoch 40, OOD Loss: 0.5138, Generator Loss: 2.8087\nEpoch 0 (Poisoned), Loss: 0.2525\nEpoch 10 (Poisoned), Loss: 0.2309\nEpoch 20 (Poisoned), Loss: 0.2175\nEpoch 30 (Poisoned), Loss: 0.1484\nEpoch 40 (Poisoned), Loss: 0.1246\nEpoch 50 (Poisoned), Loss: 0.2906\nEpoch 60 (Poisoned), Loss: 0.2775\nEpoch 70 (Poisoned), Loss: 0.2180\nEpoch 80 (Poisoned), Loss: 0.2463\nEpoch 90 (Poisoned), Loss: 0.2475\nEpoch 100 (Poisoned), Loss: 0.2644\nEpoch 110 (Poisoned), Loss: 0.2057\nEpoch 120 (Poisoned), Loss: 0.1668\nEpoch 130 (Poisoned), Loss: 0.2439\nEpoch 140 (Poisoned), Loss: 0.2256\nEpoch 150 (Poisoned), Loss: 0.2668\nEpoch 160 (Poisoned), Loss: 0.3271\nEpoch 170 (Poisoned), Loss: 0.1427\nEpoch 180 (Poisoned), Loss: 0.2434\nEpoch 190 (Poisoned), Loss: 0.2342\nGAT No Defense - Poisoned Data Accuracy: Train: 1.0000, Val: 0.7540, Test: 0.7450, ASR: 0.3750\nEpoch 0 (Defended), Loss: 1.0342\nEpoch 10 (Defended), Loss: 0.7401\nEpoch 20 (Defended), Loss: 0.0000\nEpoch 30 (Defended), Loss: 0.3613\nEpoch 40 (Defended), Loss: 0.0000\nEpoch 50 (Defended), Loss: 0.2288\nEpoch 60 (Defended), Loss: 0.1969\nEpoch 70 (Defended), Loss: 0.1731\nEpoch 80 (Defended), Loss: 0.1540\nEpoch 90 (Defended), Loss: 0.1412\nEpoch 100 (Defended), Loss: 0.1280\nEpoch 110 (Defended), Loss: 0.0000\nEpoch 120 (Defended), Loss: 0.1061\nEpoch 130 (Defended), Loss: 0.0990\nEpoch 140 (Defended), Loss: 0.0922\nEpoch 150 (Defended), Loss: 0.0853\nEpoch 160 (Defended), Loss: 0.0000\nEpoch 170 (Defended), Loss: 0.0744\nEpoch 180 (Defended), Loss: 0.0000\nEpoch 190 (Defended), Loss: 0.0000\nGAT Outlier Detection - Defended Data Accuracy: Train: 1.0000, Val: 1.0000, Test: 1.0000, ASR: 1.0000\nRunning experiment on dataset: Flickr\nRunning experiment for GCN\nEpoch 0, Loss: 2.6259\nEpoch 10, Loss: 1.7244\nEpoch 20, Loss: 1.6877\nEpoch 30, Loss: 1.6470\nEpoch 40, Loss: 1.6349\nEpoch 50, Loss: 1.6356\nEpoch 60, Loss: 1.6198\nEpoch 70, Loss: 1.6144\nEpoch 80, Loss: 1.6138\nEpoch 90, Loss: 1.6123\nEpoch 100, Loss: 1.6047\nEpoch 110, Loss: 1.6004\nEpoch 120, Loss: 1.5990\nEpoch 130, Loss: 1.6002\nEpoch 140, Loss: 1.5972\nEpoch 150, Loss: 1.5891\nEpoch 160, Loss: 1.5914\nEpoch 170, Loss: 1.5873\nEpoch 180, Loss: 1.5851\nEpoch 190, Loss: 1.5822\nGCN Clean Data Accuracy: Train: 0.4216, Val: 0.4238, Test: 0.4234\nAdversarial Epoch 0, OOD Loss: 1.3309, Generator Loss: 0.9240\nAdversarial Epoch 10, OOD Loss: 0.7836, Generator Loss: 0.6355\nAdversarial Epoch 20, OOD Loss: 0.7145, Generator Loss: 0.6828\nAdversarial Epoch 30, OOD Loss: 0.6608, Generator Loss: 0.7321\nAdversarial Epoch 40, OOD Loss: 0.6153, Generator Loss: 0.7829\nEpoch 0 (Poisoned), Loss: 1.5790\nEpoch 10 (Poisoned), Loss: 1.5803\nEpoch 20 (Poisoned), Loss: 1.5776\nEpoch 30 (Poisoned), Loss: 1.5753\nEpoch 40 (Poisoned), Loss: 1.5727\nEpoch 50 (Poisoned), Loss: 1.5701\nEpoch 60 (Poisoned), Loss: 1.5669\nEpoch 70 (Poisoned), Loss: 1.5619\nEpoch 80 (Poisoned), Loss: 1.5576\nEpoch 90 (Poisoned), Loss: 1.5525\nEpoch 100 (Poisoned), Loss: 1.5484\nEpoch 110 (Poisoned), Loss: 1.5523\nEpoch 120 (Poisoned), Loss: 1.5399\nEpoch 130 (Poisoned), Loss: 1.5394\nEpoch 140 (Poisoned), Loss: 1.5327\nEpoch 150 (Poisoned), Loss: 1.5338\nEpoch 160 (Poisoned), Loss: 1.5274\nEpoch 170 (Poisoned), Loss: 1.5303\nEpoch 180 (Poisoned), Loss: 1.5268\nEpoch 190 (Poisoned), Loss: 1.5303\nGCN No Defense - Poisoned Data Accuracy: Train: 0.4649, Val: 0.4663, Test: 0.4666, ASR: 0.0000\nEpoch 0 (Defended), Loss: 1.4129\nEpoch 10 (Defended), Loss: 1.0486\nEpoch 20 (Defended), Loss: 0.9790\nEpoch 30 (Defended), Loss: 0.7289\nEpoch 40 (Defended), Loss: 0.6419\nEpoch 50 (Defended), Loss: 0.4520\nEpoch 60 (Defended), Loss: 0.3951\nEpoch 70 (Defended), Loss: 0.2758\nEpoch 80 (Defended), Loss: 0.2342\nEpoch 90 (Defended), Loss: 0.2041\nEpoch 100 (Defended), Loss: 0.1526\nEpoch 110 (Defended), Loss: 0.1498\nEpoch 120 (Defended), Loss: 0.1608\nEpoch 130 (Defended), Loss: 0.1438\nEpoch 140 (Defended), Loss: 0.1195\nEpoch 150 (Defended), Loss: 0.1326\nEpoch 160 (Defended), Loss: 0.1128\nEpoch 170 (Defended), Loss: 0.1125\nEpoch 180 (Defended), Loss: 0.1009\nEpoch 190 (Defended), Loss: 0.1034\nGCN Outlier Detection - Defended Data Accuracy: Train: 0.9865, Val: 0.9556, Test: 0.8800, ASR: 1.0000\nRunning experiment for GraphSAGE\nEpoch 0, Loss: 2.0737\nEpoch 10, Loss: 1.9643\nEpoch 20, Loss: 1.8940\nEpoch 30, Loss: 1.8044\nEpoch 40, Loss: 1.6477\nEpoch 50, Loss: 1.6119\nEpoch 60, Loss: 1.5681\nEpoch 70, Loss: 1.5621\nEpoch 80, Loss: 1.5435\nEpoch 90, Loss: 1.5301\nEpoch 100, Loss: 1.5236\nEpoch 110, Loss: 1.5190\nEpoch 120, Loss: 1.5117\nEpoch 130, Loss: 1.5052\nEpoch 140, Loss: 1.4959\nEpoch 150, Loss: 1.5006\nEpoch 160, Loss: 1.4907\nEpoch 170, Loss: 1.4889\nEpoch 180, Loss: 1.4878\nEpoch 190, Loss: 1.4862\nGraphSAGE Clean Data Accuracy: Train: 0.5010, Val: 0.4965, Test: 0.4973\nAdversarial Epoch 0, OOD Loss: 0.5739, Generator Loss: 0.8013\nAdversarial Epoch 10, OOD Loss: 0.0304, Generator Loss: 6.8429\nAdversarial Epoch 20, OOD Loss: 0.4357, Generator Loss: 1.8246\nAdversarial Epoch 30, OOD Loss: 0.5072, Generator Loss: 0.9517\nAdversarial Epoch 40, OOD Loss: 0.0066, Generator Loss: 9.4216\nEpoch 0 (Poisoned), Loss: 1.4947\nEpoch 10 (Poisoned), Loss: 1.4805\nEpoch 20 (Poisoned), Loss: 1.4757\nEpoch 30 (Poisoned), Loss: 1.4768\nEpoch 40 (Poisoned), Loss: 1.4733\nEpoch 50 (Poisoned), Loss: 1.4743\nEpoch 60 (Poisoned), Loss: 1.4663\nEpoch 70 (Poisoned), Loss: 1.4849\nEpoch 80 (Poisoned), Loss: 1.4829\nEpoch 90 (Poisoned), Loss: 1.4765\nEpoch 100 (Poisoned), Loss: 1.4622\nEpoch 110 (Poisoned), Loss: 1.4621\nEpoch 120 (Poisoned), Loss: 1.4619\nEpoch 130 (Poisoned), Loss: 1.4579\nEpoch 140 (Poisoned), Loss: 1.4559\nEpoch 150 (Poisoned), Loss: 1.4572\nEpoch 160 (Poisoned), Loss: 1.4585\nEpoch 170 (Poisoned), Loss: 1.4825\nEpoch 180 (Poisoned), Loss: 1.4681\nEpoch 190 (Poisoned), Loss: 1.4539\nGraphSAGE No Defense - Poisoned Data Accuracy: Train: 0.5129, Val: 0.5078, Test: 0.5007, ASR: 0.0063\nEpoch 0 (Defended), Loss: 2.6854\nEpoch 10 (Defended), Loss: 1.4149\nEpoch 20 (Defended), Loss: 0.8201\nEpoch 30 (Defended), Loss: 0.7764\nEpoch 40 (Defended), Loss: 0.7064\nEpoch 50 (Defended), Loss: 0.7411\nEpoch 60 (Defended), Loss: 0.6814\nEpoch 70 (Defended), Loss: 0.5976\nEpoch 80 (Defended), Loss: 0.7912\nEpoch 90 (Defended), Loss: 0.6033\nEpoch 100 (Defended), Loss: 0.7782\nEpoch 110 (Defended), Loss: 0.6219\nEpoch 120 (Defended), Loss: 0.7022\nEpoch 130 (Defended), Loss: 0.6093\nEpoch 140 (Defended), Loss: 0.6714\nEpoch 150 (Defended), Loss: 0.7558\nEpoch 160 (Defended), Loss: 0.6438\nEpoch 170 (Defended), Loss: 0.6422\nEpoch 180 (Defended), Loss: 0.6112\nEpoch 190 (Defended), Loss: 0.6485\nGraphSAGE Outlier Detection - Defended Data Accuracy: Train: 0.8846, Val: 0.8276, Test: 0.7000, ASR: 1.0000\nRunning experiment for GAT\nEpoch 0, Loss: 17.0952\nEpoch 10, Loss: 1.8562\nEpoch 20, Loss: 1.7531\nEpoch 30, Loss: 1.7014\nEpoch 40, Loss: 1.6610\nEpoch 50, Loss: 1.6364\nEpoch 60, Loss: 1.6253\nEpoch 70, Loss: 1.6136\nEpoch 80, Loss: 1.6087\nEpoch 90, Loss: 1.5970\nEpoch 100, Loss: 1.5884\nEpoch 110, Loss: 1.5835\nEpoch 120, Loss: 1.5690\nEpoch 130, Loss: 1.5624\nEpoch 140, Loss: 1.5581\nEpoch 150, Loss: 1.5564\nEpoch 160, Loss: 1.5541\nEpoch 170, Loss: 1.5466\nEpoch 180, Loss: 1.5417\nEpoch 190, Loss: 1.5391\nGAT Clean Data Accuracy: Train: 0.4864, Val: 0.4836, Test: 0.4814\nAdversarial Epoch 0, OOD Loss: 0.2358, Generator Loss: 2.8245\nAdversarial Epoch 10, OOD Loss: 0.0239, Generator Loss: 4.2547\nAdversarial Epoch 20, OOD Loss: 0.0823, Generator Loss: 3.4617\nAdversarial Epoch 30, OOD Loss: 0.0605, Generator Loss: 4.4525\nAdversarial Epoch 40, OOD Loss: 0.0068, Generator Loss: 5.9575\nEpoch 0 (Poisoned), Loss: 1.5380\nEpoch 10 (Poisoned), Loss: 1.5353\nEpoch 20 (Poisoned), Loss: 1.5278\nEpoch 30 (Poisoned), Loss: 1.5244\nEpoch 40 (Poisoned), Loss: 1.5252\nEpoch 50 (Poisoned), Loss: 1.5217\nEpoch 60 (Poisoned), Loss: 1.5163\nEpoch 70 (Poisoned), Loss: 1.5181\nEpoch 80 (Poisoned), Loss: 1.5147\nEpoch 90 (Poisoned), Loss: 1.5196\nEpoch 100 (Poisoned), Loss: 1.5125\nEpoch 110 (Poisoned), Loss: 1.5103\nEpoch 120 (Poisoned), Loss: 1.5118\nEpoch 130 (Poisoned), Loss: 1.5102\nEpoch 140 (Poisoned), Loss: 1.5128\nEpoch 150 (Poisoned), Loss: 1.5064\nEpoch 160 (Poisoned), Loss: 1.5113\nEpoch 170 (Poisoned), Loss: 1.5066\nEpoch 180 (Poisoned), Loss: 1.5030\nEpoch 190 (Poisoned), Loss: 1.5027\nGAT No Defense - Poisoned Data Accuracy: Train: 0.5037, Val: 0.5011, Test: 0.4982, ASR: 0.0063\nEpoch 0 (Defended), Loss: 4.9501\nEpoch 10 (Defended), Loss: 1.7054\nEpoch 20 (Defended), Loss: 1.6528\nEpoch 30 (Defended), Loss: 1.6207\nEpoch 40 (Defended), Loss: 1.6326\nEpoch 50 (Defended), Loss: 1.6255\nEpoch 60 (Defended), Loss: 1.6241\nEpoch 70 (Defended), Loss: 1.6206\nEpoch 80 (Defended), Loss: 1.6816\nEpoch 90 (Defended), Loss: 1.6220\nEpoch 100 (Defended), Loss: 1.6149\nEpoch 110 (Defended), Loss: 1.6163\nEpoch 120 (Defended), Loss: 1.6198\nEpoch 130 (Defended), Loss: 1.6177\nEpoch 140 (Defended), Loss: 1.6206\nEpoch 150 (Defended), Loss: 1.6147\nEpoch 160 (Defended), Loss: 1.6184\nEpoch 170 (Defended), Loss: 1.6177\nEpoch 180 (Defended), Loss: 1.6219\nEpoch 190 (Defended), Loss: 1.6179\nGAT Outlier Detection - Defended Data Accuracy: Train: 0.4060, Val: 0.3916, Test: 0.3841, ASR: 0.0000\nRunning experiment on dataset: ogbn-arxiv\nRunning experiment for GCN\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/ogb/nodeproppred/dataset_pyg.py:69: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  self.data, self.slices = torch.load(self.processed_paths[0])\n",
          "output_type": "stream"
        },
        {
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[37], line 333\u001b[0m\n\u001b[1;32m    330\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 333\u001b[0m     \u001b[43mrun_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[37], line 244\u001b[0m, in \u001b[0;36mrun_experiment\u001b[0;34m()\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;66;03m# Train on clean data\u001b[39;00m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m--> 244\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    246\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "Cell \u001b[0;32mIn[37], line 109\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, data, optimizer)\u001b[0m\n\u001b[1;32m    107\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    108\u001b[0m out \u001b[38;5;241m=\u001b[39m model(data)\n\u001b[0;32m--> 109\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnll_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_mask\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_mask\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    110\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward(retain_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    111\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py:2778\u001b[0m, in \u001b[0;36mnll_loss\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2776\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2777\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 2778\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnll_loss_nd\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: 0D or 1D target tensor expected, multi-target not supported"
          ],
          "ename": "RuntimeError",
          "evalue": "0D or 1D target tensor expected, multi-target not supported",
          "output_type": "error"
        }
      ]
    }
  ]
}