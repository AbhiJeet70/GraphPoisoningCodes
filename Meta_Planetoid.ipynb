{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [],
      "dockerImageVersionId": 30733,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AbhiJeet70/GraphPoisoningCodes/blob/main/Meta_Planetoid.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary packages\n",
        "!pip install torch torch-geometric pandas matplotlib\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.datasets import Planetoid\n",
        "from torch_geometric.transforms import NormalizeFeatures\n",
        "from torch_geometric.nn import GCNConv\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "def set_seed(seed):\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(20)\n",
        "\n",
        "# Define the GCN model\n",
        "class GCNNet(nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
        "        super(GCNNet, self).__init__()\n",
        "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
        "        self.conv2 = GCNConv(hidden_channels, out_channels)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "class MetaLearner(nn.Module):\n",
        "    def __init__(self, model, features, adj, labels, idx_train, lambda_=0.5):\n",
        "        super(MetaLearner, self).__init__()\n",
        "        self.model = model\n",
        "        self.features = features\n",
        "        self.adj = adj\n",
        "        self.labels = labels\n",
        "        self.idx_train = idx_train\n",
        "        self.lambda_ = lambda_\n",
        "        self.adj_changes = nn.Parameter(torch.zeros_like(adj.to_dense().float()))  # Convert to float\n",
        "\n",
        "    def forward(self, adj):\n",
        "        adj = adj + self.adj_changes\n",
        "        adj = torch.clamp(adj, 0, 1)\n",
        "        return adj\n",
        "\n",
        "    def loss(self, adj, output):\n",
        "        return F.cross_entropy(output[self.idx_train], self.labels[self.idx_train])\n",
        "\n",
        "\n",
        "def meta_attack(features, edge_index, labels, idx_train, idx_unlabeled, perturbations, lambda_=0.5, epochs=100):\n",
        "    num_nodes = features.size(0)\n",
        "\n",
        "    # Convert edge_index to dense adjacency matrix\n",
        "    adj = torch.sparse_coo_tensor(edge_index, torch.ones(edge_index.size(1)), (num_nodes, num_nodes)).to_dense()\n",
        "\n",
        "    # Initialize surrogate model\n",
        "    in_channels = features.size(1)\n",
        "    hidden_channels = 64\n",
        "    out_channels = labels.max().item() + 1\n",
        "    surrogate = GCNNet(in_channels=in_channels, hidden_channels=hidden_channels, out_channels=out_channels)\n",
        "\n",
        "    # Initialize meta-learner\n",
        "    meta_learner = MetaLearner(model=surrogate, features=features, adj=adj, labels=labels, idx_train=idx_train, lambda_=lambda_)\n",
        "    optimizer = torch.optim.Adam(meta_learner.parameters(), lr=0.01)\n",
        "\n",
        "    # Perturb adjacency matrix\n",
        "    for _ in range(perturbations):\n",
        "        i = torch.randint(0, num_nodes, (1,))\n",
        "        j = torch.randint(0, num_nodes, (1,))\n",
        "        adj[i, j] = 1 - adj[i, j]\n",
        "        adj[j, i] = adj[i, j]  # Ensure symmetry\n",
        "\n",
        "    # Forward pass with surrogate model\n",
        "    surrogate.train()\n",
        "    for epoch in range(epochs):\n",
        "        optimizer.zero_grad()\n",
        "        perturbed_adj = meta_learner(adj)  # Apply meta-learner to get perturbed adjacency\n",
        "        output = surrogate(features, edge_index)\n",
        "        loss = meta_learner.loss(perturbed_adj, output)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Return the perturbed adjacency matrix as edge_index\n",
        "    edge_index_perturbed = perturbed_adj.nonzero(as_tuple=False).t().contiguous()\n",
        "    return edge_index_perturbed\n",
        "\n",
        "\n",
        "# Load data\n",
        "def load_planetoid_data(dataset_name):\n",
        "    dataset = Planetoid(root=f'/tmp/{dataset_name}', name=dataset_name, transform=NormalizeFeatures())\n",
        "    return dataset[0]\n",
        "\n",
        "# Split data indices\n",
        "def split_indices(num_nodes, train_ratio=0.7, val_ratio=0.1):\n",
        "    indices = np.random.permutation(num_nodes)\n",
        "    train_end = int(train_ratio * num_nodes)\n",
        "    val_end = int((train_ratio + val_ratio) * num_nodes)\n",
        "    return torch.tensor(indices[:train_end]), torch.tensor(indices[train_end:val_end]), torch.tensor(indices[val_end:])\n",
        "\n",
        "# Train the model\n",
        "def train_model(model, pyg_data, lr, weight_decay):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    pyg_data = pyg_data.to(device)\n",
        "    model = model.to(device)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    best_val_acc = 0\n",
        "    patience = 100\n",
        "    patience_counter = 0\n",
        "\n",
        "    for epoch in range(1, 501):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        out = model(pyg_data.x, pyg_data.edge_index)\n",
        "        loss = F.cross_entropy(out[pyg_data.train_mask], pyg_data.y[pyg_data.train_mask])\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        model.eval()\n",
        "        _, pred = model(pyg_data.x, pyg_data.edge_index).max(dim=1)\n",
        "        val_correct = float(pred[pyg_data.val_mask].eq(pyg_data.y[pyg_data.val_mask]).sum().item())\n",
        "        val_acc = val_correct / pyg_data.val_mask.sum().item()\n",
        "\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            patience_counter = 0\n",
        "            best_model_state = model.state_dict()\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "\n",
        "        if patience_counter >= patience:\n",
        "            print(f'Early stopping at epoch {epoch}')\n",
        "            break\n",
        "\n",
        "    model.load_state_dict(best_model_state)\n",
        "    model.eval()\n",
        "    _, pred = model(pyg_data.x, pyg_data.edge_index).max(dim=1)\n",
        "    correct = float(pred[pyg_data.test_mask].eq(pyg_data.y[pyg_data.test_mask]).sum().item())\n",
        "    return correct / pyg_data.test_mask.sum().item()\n",
        "\n",
        "# Define dataset statistics\n",
        "def print_dataset_statistics(data, dataset_name):\n",
        "    num_nodes = data.num_nodes\n",
        "    num_edges = data.num_edges\n",
        "    num_features = data.num_node_features\n",
        "    num_classes = data.y.max().item() + 1\n",
        "    class_distribution = torch.bincount(data.y).cpu().numpy()\n",
        "    print(f\"Statistics for {dataset_name}:\")\n",
        "    print(f\"  Number of nodes: {num_nodes}\")\n",
        "    print(f\"  Number of edges: {num_edges}\")\n",
        "    print(f\"  Number of features: {num_features}\")\n",
        "    print(f\"  Number of classes: {num_classes}\")\n",
        "    print(f\"  Class distribution: {class_distribution}\")\n",
        "\n",
        "def run_meta_attack_experiments(data, perturbation_percentage, model, best_hyperparams):\n",
        "    train_idx = data.train_mask\n",
        "    val_idx = data.val_mask\n",
        "\n",
        "    perturbations = int(perturbation_percentage * data.edge_index.size(1))\n",
        "    perturbed_data = data.clone()\n",
        "    perturbed_edge_index = meta_attack(data.x, data.edge_index, data.y, train_idx, val_idx, perturbations)\n",
        "    perturbed_data.edge_index = perturbed_edge_index\n",
        "\n",
        "    # Ensure edge_index has valid indices\n",
        "    valid_indices = (perturbed_data.edge_index[0] < perturbed_data.num_nodes) & (perturbed_data.edge_index[1] < perturbed_data.num_nodes)\n",
        "    perturbed_data.edge_index = perturbed_data.edge_index[:, valid_indices]\n",
        "\n",
        "    # Evaluate model on perturbed data\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        logits = model(perturbed_data.x, perturbed_data.edge_index)\n",
        "        pred = logits.argmax(dim=1)\n",
        "        correct = (pred[perturbed_data.test_mask] == perturbed_data.y[perturbed_data.test_mask]).sum().item()\n",
        "        acc = correct / perturbed_data.test_mask.sum().item()\n",
        "\n",
        "    return acc\n",
        "\n",
        "# Hyperparameter grid search\n",
        "hidden_channels_list = [16, 32, 64, 128, 256, 512]  # Reduced for quicker testing\n",
        "learning_rates = [0.1, 0.01, 0.001]  # Reduced for quicker testing\n",
        "weight_decays = [1e-4, 1e-5]\n",
        "\n",
        "# List of datasets to process\n",
        "datasets = ['Cora', 'Pubmed', 'CiteSeer']\n",
        "\n",
        "# Initialize results DataFrame\n",
        "results_df = pd.DataFrame(columns=['Dataset', 'Hidden_Channels', 'Learning_Rate', 'Weight_Decay', 'Accuracy', 'Perturbation_Type', 'Perturbation_Percentage'])\n",
        "\n",
        "# Device setup\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Update the code that initializes the model\n",
        "for dataset_name in datasets:\n",
        "    print(f'Processing dataset: {dataset_name}')\n",
        "    data = load_planetoid_data(dataset_name)\n",
        "    print_dataset_statistics(data, dataset_name)\n",
        "\n",
        "    train_idx, val_idx, test_idx = split_indices(data.num_nodes)\n",
        "    data.train_mask = torch.zeros(data.num_nodes, dtype=torch.bool).to(device)\n",
        "    data.val_mask = torch.zeros(data.num_nodes, dtype=torch.bool).to(device)\n",
        "    data.test_mask = torch.zeros(data.num_nodes, dtype=torch.bool).to(device)\n",
        "    data.train_mask[train_idx] = True\n",
        "    data.val_mask[val_idx] = True\n",
        "    data.test_mask[test_idx] = True\n",
        "\n",
        "    num_classes = data.y.max().item() + 1  # Calculate the number of classes\n",
        "\n",
        "    best_accuracy = 0\n",
        "    best_hyperparams = {'Hidden_Channels': None, 'Learning_Rate': None, 'Weight_Decay': None}\n",
        "\n",
        "    for hidden_channels in hidden_channels_list:\n",
        "        for lr in learning_rates:\n",
        "            for wd in weight_decays:\n",
        "                model = GCNNet(data.num_node_features, hidden_channels, num_classes)\n",
        "                accuracy = train_model(model, data, lr, wd)\n",
        "                print(f'Hidden Channels: {hidden_channels}, Learning Rate: {lr}, Weight Decay: {wd}, Accuracy: {accuracy}')\n",
        "\n",
        "                if accuracy > best_accuracy:\n",
        "                    best_accuracy = accuracy\n",
        "                    best_hyperparams = {'Hidden_Channels': hidden_channels, 'Learning_Rate': lr, 'Weight_Decay': wd}\n",
        "\n",
        "    print(f'Best Hyperparameters for {dataset_name}: {best_hyperparams}, Accuracy: {best_accuracy}')\n",
        "\n",
        "    # Run meta-attack experiments for different perturbation percentages\n",
        "    perturbation_percentages = [0.05, 0.1, 0.15, 0.2, 0.25]\n",
        "    perturbation_accuracies = []\n",
        "\n",
        "    for perturbation_percentage in perturbation_percentages:\n",
        "        acc = run_meta_attack_experiments(data, perturbation_percentage, model, best_hyperparams)\n",
        "        perturbation_accuracies.append(acc)\n",
        "        new_row = pd.DataFrame({\n",
        "            'Dataset': [dataset_name],\n",
        "            'Hidden_Channels': [best_hyperparams['Hidden_Channels']],\n",
        "            'Learning_Rate': [best_hyperparams['Learning_Rate']],\n",
        "            'Weight_Decay': [best_hyperparams['Weight_Decay']],\n",
        "            'Accuracy': [acc],\n",
        "            'Perturbation_Type': ['Meta'],\n",
        "            'Perturbation_Percentage': [perturbation_percentage]\n",
        "        })\n",
        "        results_df = pd.concat([results_df, new_row], ignore_index=True)\n",
        "\n",
        "        print(f'Perturbation Percentage: {perturbation_percentage}, Accuracy: {acc}')\n",
        "\n",
        "\n",
        "# Save results to CSV\n",
        "results_df.to_csv('meta_attack_results.csv', index=False)\n",
        "\n",
        "print('Experiments completed and results saved to meta_attack_results.csv.')\n",
        "\n"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2024-07-25T07:33:18.297826Z",
          "iopub.execute_input": "2024-07-25T07:33:18.298285Z",
          "iopub.status.idle": "2024-07-25T09:43:42.825423Z",
          "shell.execute_reply.started": "2024-07-25T07:33:18.29825Z",
          "shell.execute_reply": "2024-07-25T09:43:42.823688Z"
        },
        "trusted": true,
        "id": "rA1pSVZOqFmf",
        "outputId": "1a9d5d9f-e24d-4dad-aafe-6306761bcb11"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.1.2+cpu)\nRequirement already satisfied: torch-geometric in /opt/conda/lib/python3.10/site-packages (2.5.3)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (2.2.2)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (3.7.5)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.12.1)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2024.3.1)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from torch-geometric) (4.66.4)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torch-geometric) (1.26.4)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from torch-geometric) (1.11.4)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from torch-geometric) (3.9.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torch-geometric) (2.32.3)\nRequirement already satisfied: pyparsing in /opt/conda/lib/python3.10/site-packages (from torch-geometric) (3.1.1)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from torch-geometric) (1.2.2)\nRequirement already satisfied: psutil>=5.8.0 in /opt/conda/lib/python3.10/site-packages (from torch-geometric) (5.9.3)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas) (2023.4)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (1.2.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (4.47.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (1.4.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (21.3)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (9.5.0)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch-geometric) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch-geometric) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch-geometric) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch-geometric) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch-geometric) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch-geometric) (4.0.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torch-geometric) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torch-geometric) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torch-geometric) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torch-geometric) (2024.2.2)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->torch-geometric) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->torch-geometric) (3.2.0)\nRequirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\nProcessing dataset: Cora\nStatistics for Cora:\n  Number of nodes: 2708\n  Number of edges: 10556\n  Number of features: 1433\n  Number of classes: 7\n  Class distribution: [351 217 418 818 426 298 180]\nEarly stopping at epoch 147\nHidden Channels: 16, Learning Rate: 0.1, Weight Decay: 0.0001, Accuracy: 0.8819188191881919\nEarly stopping at epoch 159\nHidden Channels: 16, Learning Rate: 0.1, Weight Decay: 1e-05, Accuracy: 0.8892988929889298\nEarly stopping at epoch 191\nHidden Channels: 16, Learning Rate: 0.01, Weight Decay: 0.0001, Accuracy: 0.8892988929889298\nEarly stopping at epoch 249\nHidden Channels: 16, Learning Rate: 0.01, Weight Decay: 1e-05, Accuracy: 0.8874538745387454\nEarly stopping at epoch 103\nHidden Channels: 16, Learning Rate: 0.001, Weight Decay: 0.0001, Accuracy: 0.2730627306273063\nEarly stopping at epoch 112\nHidden Channels: 16, Learning Rate: 0.001, Weight Decay: 1e-05, Accuracy: 0.27121771217712176\nEarly stopping at epoch 133\nHidden Channels: 32, Learning Rate: 0.1, Weight Decay: 0.0001, Accuracy: 0.8763837638376384\nEarly stopping at epoch 135\nHidden Channels: 32, Learning Rate: 0.1, Weight Decay: 1e-05, Accuracy: 0.8837638376383764\nEarly stopping at epoch 211\nHidden Channels: 32, Learning Rate: 0.01, Weight Decay: 0.0001, Accuracy: 0.8856088560885609\nEarly stopping at epoch 277\nHidden Channels: 32, Learning Rate: 0.01, Weight Decay: 1e-05, Accuracy: 0.8837638376383764\nHidden Channels: 32, Learning Rate: 0.001, Weight Decay: 0.0001, Accuracy: 0.8911439114391144\nHidden Channels: 32, Learning Rate: 0.001, Weight Decay: 1e-05, Accuracy: 0.8819188191881919\nEarly stopping at epoch 128\nHidden Channels: 64, Learning Rate: 0.1, Weight Decay: 0.0001, Accuracy: 0.8671586715867159\nEarly stopping at epoch 122\nHidden Channels: 64, Learning Rate: 0.1, Weight Decay: 1e-05, Accuracy: 0.8634686346863468\nEarly stopping at epoch 183\nHidden Channels: 64, Learning Rate: 0.01, Weight Decay: 0.0001, Accuracy: 0.8837638376383764\nEarly stopping at epoch 179\nHidden Channels: 64, Learning Rate: 0.01, Weight Decay: 1e-05, Accuracy: 0.8837638376383764\nHidden Channels: 64, Learning Rate: 0.001, Weight Decay: 0.0001, Accuracy: 0.8966789667896679\nHidden Channels: 64, Learning Rate: 0.001, Weight Decay: 1e-05, Accuracy: 0.8948339483394834\nEarly stopping at epoch 114\nHidden Channels: 128, Learning Rate: 0.1, Weight Decay: 0.0001, Accuracy: 0.8653136531365314\nEarly stopping at epoch 129\nHidden Channels: 128, Learning Rate: 0.1, Weight Decay: 1e-05, Accuracy: 0.8671586715867159\nEarly stopping at epoch 160\nHidden Channels: 128, Learning Rate: 0.01, Weight Decay: 0.0001, Accuracy: 0.8819188191881919\nEarly stopping at epoch 168\nHidden Channels: 128, Learning Rate: 0.01, Weight Decay: 1e-05, Accuracy: 0.8800738007380073\nEarly stopping at epoch 441\nHidden Channels: 128, Learning Rate: 0.001, Weight Decay: 0.0001, Accuracy: 0.8929889298892989\nEarly stopping at epoch 408\nHidden Channels: 128, Learning Rate: 0.001, Weight Decay: 1e-05, Accuracy: 0.8948339483394834\nEarly stopping at epoch 136\nHidden Channels: 256, Learning Rate: 0.1, Weight Decay: 0.0001, Accuracy: 0.8653136531365314\nEarly stopping at epoch 130\nHidden Channels: 256, Learning Rate: 0.1, Weight Decay: 1e-05, Accuracy: 0.8671586715867159\nEarly stopping at epoch 152\nHidden Channels: 256, Learning Rate: 0.01, Weight Decay: 0.0001, Accuracy: 0.8763837638376384\nEarly stopping at epoch 152\nHidden Channels: 256, Learning Rate: 0.01, Weight Decay: 1e-05, Accuracy: 0.8800738007380073\nEarly stopping at epoch 405\nHidden Channels: 256, Learning Rate: 0.001, Weight Decay: 0.0001, Accuracy: 0.8929889298892989\nEarly stopping at epoch 382\nHidden Channels: 256, Learning Rate: 0.001, Weight Decay: 1e-05, Accuracy: 0.8892988929889298\nEarly stopping at epoch 128\nHidden Channels: 512, Learning Rate: 0.1, Weight Decay: 0.0001, Accuracy: 0.8708487084870848\nEarly stopping at epoch 131\nHidden Channels: 512, Learning Rate: 0.1, Weight Decay: 1e-05, Accuracy: 0.8690036900369004\nEarly stopping at epoch 144\nHidden Channels: 512, Learning Rate: 0.01, Weight Decay: 0.0001, Accuracy: 0.8708487084870848\nEarly stopping at epoch 141\nHidden Channels: 512, Learning Rate: 0.01, Weight Decay: 1e-05, Accuracy: 0.8745387453874539\nEarly stopping at epoch 296\nHidden Channels: 512, Learning Rate: 0.001, Weight Decay: 0.0001, Accuracy: 0.8929889298892989\nEarly stopping at epoch 260\nHidden Channels: 512, Learning Rate: 0.001, Weight Decay: 1e-05, Accuracy: 0.8892988929889298\nBest Hyperparameters for Cora: {'Hidden_Channels': 64, 'Learning_Rate': 0.001, 'Weight_Decay': 0.0001}, Accuracy: 0.8966789667896679\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/tmp/ipykernel_33/3688731769.py:252: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n  results_df = pd.concat([results_df, new_row], ignore_index=True)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Perturbation Percentage: 0.05, Accuracy: 0.8763837638376384\nPerturbation Percentage: 0.1, Accuracy: 0.8726937269372693\nPerturbation Percentage: 0.15, Accuracy: 0.8634686346863468\nPerturbation Percentage: 0.2, Accuracy: 0.8210332103321033\nPerturbation Percentage: 0.25, Accuracy: 0.7822878228782287\nProcessing dataset: Pubmed\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.x\nDownloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.tx\nDownloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.allx\nDownloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.y\nDownloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.ty\nDownloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.ally\nDownloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.graph\nDownloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.test.index\nProcessing...\nDone!\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Statistics for Pubmed:\n  Number of nodes: 19717\n  Number of edges: 88648\n  Number of features: 500\n  Number of classes: 3\n  Class distribution: [4103 7739 7875]\nEarly stopping at epoch 288\nHidden Channels: 16, Learning Rate: 0.1, Weight Decay: 0.0001, Accuracy: 0.8803245436105477\nEarly stopping at epoch 261\nHidden Channels: 16, Learning Rate: 0.1, Weight Decay: 1e-05, Accuracy: 0.8770283975659229\nHidden Channels: 16, Learning Rate: 0.01, Weight Decay: 0.0001, Accuracy: 0.8795638945233266\nHidden Channels: 16, Learning Rate: 0.01, Weight Decay: 1e-05, Accuracy: 0.880578093306288\nHidden Channels: 16, Learning Rate: 0.001, Weight Decay: 0.0001, Accuracy: 0.8263184584178499\nHidden Channels: 16, Learning Rate: 0.001, Weight Decay: 1e-05, Accuracy: 0.8286004056795132\nEarly stopping at epoch 478\nHidden Channels: 32, Learning Rate: 0.1, Weight Decay: 0.0001, Accuracy: 0.8780425963488844\nEarly stopping at epoch 256\nHidden Channels: 32, Learning Rate: 0.1, Weight Decay: 1e-05, Accuracy: 0.8790567951318459\nEarly stopping at epoch 490\nHidden Channels: 32, Learning Rate: 0.01, Weight Decay: 0.0001, Accuracy: 0.8813387423935092\nHidden Channels: 32, Learning Rate: 0.01, Weight Decay: 1e-05, Accuracy: 0.8846348884381339\nHidden Channels: 32, Learning Rate: 0.001, Weight Decay: 0.0001, Accuracy: 0.8412778904665315\nHidden Channels: 32, Learning Rate: 0.001, Weight Decay: 1e-05, Accuracy: 0.8438133874239351\nEarly stopping at epoch 206\nHidden Channels: 64, Learning Rate: 0.1, Weight Decay: 0.0001, Accuracy: 0.8818458417849898\nEarly stopping at epoch 203\nHidden Channels: 64, Learning Rate: 0.1, Weight Decay: 1e-05, Accuracy: 0.8785496957403651\nHidden Channels: 64, Learning Rate: 0.01, Weight Decay: 0.0001, Accuracy: 0.8859026369168357\nHidden Channels: 64, Learning Rate: 0.01, Weight Decay: 1e-05, Accuracy: 0.8859026369168357\nHidden Channels: 64, Learning Rate: 0.001, Weight Decay: 0.0001, Accuracy: 0.8501521298174443\nHidden Channels: 64, Learning Rate: 0.001, Weight Decay: 1e-05, Accuracy: 0.8519269776876268\nEarly stopping at epoch 271\nHidden Channels: 128, Learning Rate: 0.1, Weight Decay: 0.0001, Accuracy: 0.8798174442190669\nEarly stopping at epoch 191\nHidden Channels: 128, Learning Rate: 0.1, Weight Decay: 1e-05, Accuracy: 0.8772819472616633\nHidden Channels: 128, Learning Rate: 0.01, Weight Decay: 0.0001, Accuracy: 0.8836206896551724\nEarly stopping at epoch 317\nHidden Channels: 128, Learning Rate: 0.01, Weight Decay: 1e-05, Accuracy: 0.8861561866125761\nHidden Channels: 128, Learning Rate: 0.001, Weight Decay: 0.0001, Accuracy: 0.8569979716024341\nHidden Channels: 128, Learning Rate: 0.001, Weight Decay: 1e-05, Accuracy: 0.8587728194726166\nEarly stopping at epoch 185\nHidden Channels: 256, Learning Rate: 0.1, Weight Decay: 0.0001, Accuracy: 0.8803245436105477\nEarly stopping at epoch 241\nHidden Channels: 256, Learning Rate: 0.1, Weight Decay: 1e-05, Accuracy: 0.882606490872211\nEarly stopping at epoch 466\nHidden Channels: 256, Learning Rate: 0.01, Weight Decay: 0.0001, Accuracy: 0.8798174442190669\nHidden Channels: 256, Learning Rate: 0.01, Weight Decay: 1e-05, Accuracy: 0.8818458417849898\nHidden Channels: 256, Learning Rate: 0.001, Weight Decay: 0.0001, Accuracy: 0.8643509127789046\nHidden Channels: 256, Learning Rate: 0.001, Weight Decay: 1e-05, Accuracy: 0.8640973630831643\nEarly stopping at epoch 261\nHidden Channels: 512, Learning Rate: 0.1, Weight Decay: 0.0001, Accuracy: 0.8762677484787018\nEarly stopping at epoch 208\nHidden Channels: 512, Learning Rate: 0.1, Weight Decay: 1e-05, Accuracy: 0.8788032454361054\nEarly stopping at epoch 353\nHidden Channels: 512, Learning Rate: 0.01, Weight Decay: 0.0001, Accuracy: 0.8841277890466531\nEarly stopping at epoch 377\nHidden Channels: 512, Learning Rate: 0.01, Weight Decay: 1e-05, Accuracy: 0.880578093306288\nHidden Channels: 512, Learning Rate: 0.001, Weight Decay: 0.0001, Accuracy: 0.8689148073022313\nHidden Channels: 512, Learning Rate: 0.001, Weight Decay: 1e-05, Accuracy: 0.8689148073022313\nBest Hyperparameters for Pubmed: {'Hidden_Channels': 128, 'Learning_Rate': 0.01, 'Weight_Decay': 1e-05}, Accuracy: 0.8861561866125761\nPerturbation Percentage: 0.05, Accuracy: 0.8468559837728195\nPerturbation Percentage: 0.1, Accuracy: 0.8278397565922921\nPerturbation Percentage: 0.15, Accuracy: 0.8103448275862069\nPerturbation Percentage: 0.2, Accuracy: 0.7953853955375254\nPerturbation Percentage: 0.25, Accuracy: 0.7730730223123732\nProcessing dataset: CiteSeer\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.x\nDownloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.tx\nDownloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.allx\nDownloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.y\nDownloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.ty\nDownloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.ally\nDownloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.graph\nDownloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.test.index\nProcessing...\nDone!\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Statistics for CiteSeer:\n  Number of nodes: 3327\n  Number of edges: 9104\n  Number of features: 3703\n  Number of classes: 6\n  Class distribution: [264 590 668 701 596 508]\nEarly stopping at epoch 128\nHidden Channels: 16, Learning Rate: 0.1, Weight Decay: 0.0001, Accuracy: 0.7597597597597597\nEarly stopping at epoch 118\nHidden Channels: 16, Learning Rate: 0.1, Weight Decay: 1e-05, Accuracy: 0.7732732732732732\nEarly stopping at epoch 209\nHidden Channels: 16, Learning Rate: 0.01, Weight Decay: 0.0001, Accuracy: 0.7852852852852853\nEarly stopping at epoch 263\nHidden Channels: 16, Learning Rate: 0.01, Weight Decay: 1e-05, Accuracy: 0.7687687687687688\nHidden Channels: 16, Learning Rate: 0.001, Weight Decay: 0.0001, Accuracy: 0.7822822822822822\nHidden Channels: 16, Learning Rate: 0.001, Weight Decay: 1e-05, Accuracy: 0.7807807807807807\nEarly stopping at epoch 116\nHidden Channels: 32, Learning Rate: 0.1, Weight Decay: 0.0001, Accuracy: 0.7612612612612613\nEarly stopping at epoch 116\nHidden Channels: 32, Learning Rate: 0.1, Weight Decay: 1e-05, Accuracy: 0.7612612612612613\nEarly stopping at epoch 174\nHidden Channels: 32, Learning Rate: 0.01, Weight Decay: 0.0001, Accuracy: 0.7672672672672672\nEarly stopping at epoch 243\nHidden Channels: 32, Learning Rate: 0.01, Weight Decay: 1e-05, Accuracy: 0.7762762762762763\nHidden Channels: 32, Learning Rate: 0.001, Weight Decay: 0.0001, Accuracy: 0.7957957957957958\nHidden Channels: 32, Learning Rate: 0.001, Weight Decay: 1e-05, Accuracy: 0.7897897897897898\nEarly stopping at epoch 110\nHidden Channels: 64, Learning Rate: 0.1, Weight Decay: 0.0001, Accuracy: 0.7687687687687688\nEarly stopping at epoch 112\nHidden Channels: 64, Learning Rate: 0.1, Weight Decay: 1e-05, Accuracy: 0.7567567567567568\nEarly stopping at epoch 144\nHidden Channels: 64, Learning Rate: 0.01, Weight Decay: 0.0001, Accuracy: 0.7747747747747747\nEarly stopping at epoch 162\nHidden Channels: 64, Learning Rate: 0.01, Weight Decay: 1e-05, Accuracy: 0.7687687687687688\nEarly stopping at epoch 462\nHidden Channels: 64, Learning Rate: 0.001, Weight Decay: 0.0001, Accuracy: 0.7927927927927928\nHidden Channels: 64, Learning Rate: 0.001, Weight Decay: 1e-05, Accuracy: 0.7957957957957958\nEarly stopping at epoch 110\nHidden Channels: 128, Learning Rate: 0.1, Weight Decay: 0.0001, Accuracy: 0.7567567567567568\nEarly stopping at epoch 115\nHidden Channels: 128, Learning Rate: 0.1, Weight Decay: 1e-05, Accuracy: 0.7642642642642643\nEarly stopping at epoch 145\nHidden Channels: 128, Learning Rate: 0.01, Weight Decay: 0.0001, Accuracy: 0.7702702702702703\nEarly stopping at epoch 136\nHidden Channels: 128, Learning Rate: 0.01, Weight Decay: 1e-05, Accuracy: 0.7732732732732732\nEarly stopping at epoch 379\nHidden Channels: 128, Learning Rate: 0.001, Weight Decay: 0.0001, Accuracy: 0.7942942942942943\nEarly stopping at epoch 376\nHidden Channels: 128, Learning Rate: 0.001, Weight Decay: 1e-05, Accuracy: 0.7972972972972973\nEarly stopping at epoch 112\nHidden Channels: 256, Learning Rate: 0.1, Weight Decay: 0.0001, Accuracy: 0.7642642642642643\nEarly stopping at epoch 115\nHidden Channels: 256, Learning Rate: 0.1, Weight Decay: 1e-05, Accuracy: 0.7657657657657657\nEarly stopping at epoch 136\nHidden Channels: 256, Learning Rate: 0.01, Weight Decay: 0.0001, Accuracy: 0.7642642642642643\nEarly stopping at epoch 137\nHidden Channels: 256, Learning Rate: 0.01, Weight Decay: 1e-05, Accuracy: 0.7672672672672672\nEarly stopping at epoch 277\nHidden Channels: 256, Learning Rate: 0.001, Weight Decay: 0.0001, Accuracy: 0.7942942942942943\nEarly stopping at epoch 305\nHidden Channels: 256, Learning Rate: 0.001, Weight Decay: 1e-05, Accuracy: 0.7897897897897898\nEarly stopping at epoch 116\nHidden Channels: 512, Learning Rate: 0.1, Weight Decay: 0.0001, Accuracy: 0.7612612612612613\nEarly stopping at epoch 117\nHidden Channels: 512, Learning Rate: 0.1, Weight Decay: 1e-05, Accuracy: 0.7582582582582582\nEarly stopping at epoch 123\nHidden Channels: 512, Learning Rate: 0.01, Weight Decay: 0.0001, Accuracy: 0.7672672672672672\nEarly stopping at epoch 125\nHidden Channels: 512, Learning Rate: 0.01, Weight Decay: 1e-05, Accuracy: 0.7657657657657657\nEarly stopping at epoch 229\nHidden Channels: 512, Learning Rate: 0.001, Weight Decay: 0.0001, Accuracy: 0.7897897897897898\nEarly stopping at epoch 243\nHidden Channels: 512, Learning Rate: 0.001, Weight Decay: 1e-05, Accuracy: 0.7897897897897898\nBest Hyperparameters for CiteSeer: {'Hidden_Channels': 128, 'Learning_Rate': 0.001, 'Weight_Decay': 1e-05}, Accuracy: 0.7972972972972973\nPerturbation Percentage: 0.05, Accuracy: 0.7717717717717718\nPerturbation Percentage: 0.1, Accuracy: 0.7702702702702703\nPerturbation Percentage: 0.15, Accuracy: 0.7507507507507507\nPerturbation Percentage: 0.2, Accuracy: 0.7492492492492493\nPerturbation Percentage: 0.25, Accuracy: 0.7537537537537538\nExperiments completed and results saved to meta_attack_results.csv.\n",
          "output_type": "stream"
        }
      ]
    }
  ]
}